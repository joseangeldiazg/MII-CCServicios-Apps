#Práctica 5

Ciencia de Datos con Hadoop

## Enunciado y objetivos. 

El objetivo de esta práctica es conocer las alternativas para realizar experimentaciones de Ciencia de Datos. Para ello, haremos uso del entorno que se ha convertido en un estándar de facto como es Hadoop, utilizando HDFS como sistema de archivos distribuido y Hadoop- MapReduce como mecanismo de ejecución. Por último, aplicaremos la biblioteca Mahout para lanzar algoritmos de clasificación sobre conjuntos tipo Big Data.


Los objetivos serán:

	
- Ejecutar el algoritmo “Random Forest” sobre el conjunto de datos BNG_heart y comprueba el rendimiento alcanzado de acuerdo a los siguientes casos:

	- Número de Maps: 64, 128, 256
	- Número de árboles: 10, 100, 1000

- Obtener una tabla de resultados sobre las combinaciones anteriores que contenga los siguientes datos:

	- Características del modelo: número de nodos (total y promedio), profundidad máxima del árbol.	- Tiempo de ejecución para entrenamiento.	-  Medidas de calidad Accuracy estándar y media geométrica tanto para la partición de entrenamiento como para test.

## Ejecución 

## Tabla de resultados	
	
## Conlusiones		